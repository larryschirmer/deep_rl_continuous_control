{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from time import perf_counter\n",
    "import pandas as pd\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from helpers import save_model, plot_losses, plot_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='Reacher_20.app')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.shared_linear0 = nn.Linear(params['input_dim'], params['shared_hidden0'])\n",
    "        self.shared_linear1 = nn.Linear(params['shared_hidden0'], params['shared_hidden1'])\n",
    "        self.shared_linear2 = nn.Linear(params['shared_hidden1'], params['shared_hidden2'])\n",
    "\n",
    "        self.actor_linear0 = nn.Linear(params['shared_hidden2'], params['actor_hidden'])\n",
    "        self.actor_linear1 = nn.Linear(params['actor_hidden'], params['actor_hidden'])\n",
    "        self.actor_linear2 = nn.Linear(params['actor_hidden'], params['output_dim_actor'])\n",
    "\n",
    "        self.critic_linear0 = nn.Linear(params['shared_hidden2'], params['critic_hidden'])\n",
    "        self.critic_linear1 = nn.Linear(params['critic_hidden'], params['critic_hidden'])\n",
    "        self.critic_linear2 = nn.Linear(params['critic_hidden'], params['output_dim_critic'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.tanh(self.shared_linear0(x))\n",
    "        y = torch.tanh(self.shared_linear1(y))\n",
    "        y = torch.tanh(self.shared_linear2(y))\n",
    "\n",
    "        a = torch.tanh(self.actor_linear0(y))\n",
    "        a = torch.tanh(self.actor_linear1(a))\n",
    "        actor = self.actor_linear2(a)\n",
    "        actor_mean = torch.tanh(actor)\n",
    "\n",
    "        c = torch.relu(self.critic_linear0(y.detach()))\n",
    "        c = torch.relu(self.critic_linear1(c))\n",
    "        critic = torch.relu(self.critic_linear2(c))\n",
    "        return actor_mean, critic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "epochs = 30000\n",
    "lr = 0.00008\n",
    "gamma = 0.99\n",
    "clc = 0.1\n",
    "start_epsilon = 0.3\n",
    "end_epsilon = 0.1\n",
    "start_reward_leadup = 50\n",
    "end_reward_leadup = 1\n",
    "batch_size = 40\n",
    "\n",
    "input_dim = 33\n",
    "shared_hidden0 = 64\n",
    "shared_hidden1 = 128\n",
    "shared_hidden2 = 64\n",
    "actor_hidden = 32\n",
    "critic_hidden = 32\n",
    "output_dim_actor = 4\n",
    "output_dim_critic = 1\n",
    "\n",
    "losses = []\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "scores = []\n",
    "ave_scores = []\n",
    "\n",
    "params = {\n",
    "    'env': env,\n",
    "    'brain_name': brain_name,\n",
    "    'start_epsilon': start_epsilon,\n",
    "    'end_epsilon': end_epsilon,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'gamma': gamma,\n",
    "    'clc': clc,\n",
    "    'start_reward_leadup': start_reward_leadup,\n",
    "    'end_reward_leadup': end_reward_leadup,\n",
    "    'batch_size': batch_size,\n",
    "    'losses': losses,\n",
    "    'scores': scores,\n",
    "    'ave_scores': ave_scores,\n",
    "    'actor_losses': actor_losses,\n",
    "    'critic_losses': critic_losses\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    'input_dim': input_dim,\n",
    "    'shared_hidden0': shared_hidden0,\n",
    "    'shared_hidden1': shared_hidden1,\n",
    "    'shared_hidden2': shared_hidden2,\n",
    "    'critic_hidden': critic_hidden,\n",
    "    'actor_hidden': actor_hidden,\n",
    "    'output_dim_actor': output_dim_actor,\n",
    "    'output_dim_critic': output_dim_critic\n",
    "}\n",
    "\n",
    "model = ActorCritic(model_params)\n",
    "optimizer = torch.optim.Adam(lr=params['lr'], params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(model, optimizer, params, train=True, early_stop_threshold=5., early_stop_target=30.):\n",
    "\n",
    "    replay = []\n",
    "\n",
    "    highest_score = 0\n",
    "    early_stop_captures = []\n",
    "\n",
    "    for epoch in range(params['epochs']):\n",
    "        if train and len(early_stop_captures) >= early_stop_threshold:\n",
    "            print(\"stopped early because net has reached target score\")\n",
    "            print(early_stop_captures)\n",
    "            break\n",
    "\n",
    "        final_score, epsilon, reward_leadup = run_episode(model, replay, params, epoch, train)\n",
    "        params['scores'].append(final_score)\n",
    "        stacked_scores = np.stack(params['scores'], axis=1)\n",
    "        sliced_scores = [agent_scores[-100:] for agent_scores in stacked_scores]\n",
    "        average_score = np.mean(sliced_scores, axis=1)\n",
    "        params['ave_scores'].append(average_score)\n",
    "\n",
    "        if train and len(replay) >= params['batch_size']:\n",
    "            loss, actor_loss, critic_loss = update_params(replay, optimizer, params)\n",
    "\n",
    "            params['losses'].append(loss.item())\n",
    "            params['actor_losses'].append(actor_loss.item())\n",
    "            params['critic_losses'].append(critic_loss.item())\n",
    "\n",
    "            ave_scores = np.array2string(average_score, separator=', ', formatter={'float_kind':'{:.3f}'.format}, max_line_width=70).strip('[]')\n",
    "\n",
    "            if (1 + epoch) % 10 == 0:\n",
    "                print(\"Epoch: {}, Epsilon: {:.3f}, Reward Leadup: {:.1f}, Max: {:.4f}\\nAve Scores:\\n {}\\n\".format(epoch + 1, epsilon, reward_leadup, np.amax(params['scores']), ave_scores))\n",
    "        \n",
    "            replay = []\n",
    "            early_stop_compare_array = np.full((len(average_score),), early_stop_target, dtype=float)\n",
    "            if np.all(np.greater(average_score, early_stop_compare_array)):\n",
    "                early_stop_captures.append(average_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(model, replay, params, epoch, train):\n",
    "\n",
    "    env_info = params['env'].reset(train_mode=train)[params['brain_name']]\n",
    "    state_ = env_info.vector_observations\n",
    "    num_agents = len(env_info.agents)\n",
    "    states = torch.from_numpy(state_).float()\n",
    "    scores = np.zeros(num_agents)               # initialize the score\n",
    "\n",
    "    values, logprobs, rewards, mean_entropy = [], [], [], torch.tensor(0.)\n",
    "    done = False\n",
    "\n",
    "    epsilon = np.clip((params['end_epsilon'] - params['start_epsilon']) / (params['epochs'] - 0) * epoch + params['start_epsilon'], params['end_epsilon'], params['start_epsilon'])\n",
    "    step_count = 0\n",
    "    while (done == False):\n",
    "        step_count += 1\n",
    "        actor_mean, value = model(states)\n",
    "        actor_std = torch.tensor(epsilon)\n",
    "\n",
    "        actor_mean = actor_mean.t()\n",
    "\n",
    "        action_dist0 = torch.distributions.Normal(actor_mean[0], actor_std)\n",
    "        action_dist1 = torch.distributions.Normal(actor_mean[1], actor_std)\n",
    "        action_dist2 = torch.distributions.Normal(actor_mean[2], actor_std)\n",
    "        action_dist3 = torch.distributions.Normal(actor_mean[3], actor_std)\n",
    "\n",
    "        mean_entropy = action_dist0.entropy().mean()\n",
    "\n",
    "        action0 = torch.clamp(action_dist0.sample(), min=-1, max=1)\n",
    "        action1 = torch.clamp(action_dist1.sample(), min=-1, max=1)\n",
    "        action2 = torch.clamp(action_dist2.sample(), min=-1, max=1)\n",
    "        action3 = torch.clamp(action_dist3.sample(), min=-1, max=1)\n",
    "        logprob0 = action_dist0.log_prob(action0)\n",
    "        logprob1 = action_dist1.log_prob(action1)\n",
    "        logprob2 = action_dist2.log_prob(action2)\n",
    "        logprob3 = action_dist3.log_prob(action3)\n",
    "\n",
    "        values.append(value.view(-1))\n",
    "        logprobs.append([logprob0.view(-1), logprob1.view(-1), logprob2.view(-1), logprob3.view(-1)])\n",
    "\n",
    "        action_list = [action0.detach().numpy().squeeze(), action1.detach().numpy().squeeze(), action2.detach().numpy().squeeze(), action3.detach().numpy().squeeze()]\n",
    "        action_list = np.stack(action_list, axis=1)\n",
    "        # send all actions to the environment\n",
    "        env_info = params['env'].step(action_list)[params['brain_name']]\n",
    "        # get next state (for each agent)\n",
    "        state_ = env_info.vector_observations\n",
    "        # get reward (for each agent)\n",
    "        reward = env_info.rewards\n",
    "        # see if episode finished\n",
    "        done = env_info.local_done[0]\n",
    "\n",
    "        states = torch.from_numpy(state_).float()\n",
    "        rewards.append(reward)\n",
    "        scores += np.array(reward)\n",
    "\n",
    "\n",
    "    # Update replay buffer for each agent\n",
    "\n",
    "\n",
    "    stacked_logprob0 = torch.stack([a[0] for a in logprobs], dim=1)\n",
    "    stacked_logprob1 = torch.stack([a[1] for a in logprobs], dim=1)\n",
    "    stacked_logprob2 = torch.stack([a[2] for a in logprobs], dim=1)\n",
    "    stacked_logprob3 = torch.stack([a[3] for a in logprobs], dim=1)\n",
    "\n",
    "    stacked_values = torch.stack(values, dim=1)\n",
    "    stacked_rewards = np.stack(rewards, axis=1)\n",
    "\n",
    "    for agent_index in range(len(env_info.agents)):\n",
    "  \n",
    "        agent_values = stacked_values[agent_index]\n",
    "        agent_logprobs = [stacked_logprob0[agent_index], stacked_logprob1[agent_index], stacked_logprob2[agent_index], stacked_logprob3[agent_index]]\n",
    "        agent_rewards = stacked_rewards[agent_index]\n",
    "\n",
    "        actor_losses, critic_losses, losses, reward_leadup = get_trjectory_loss(agent_values, agent_logprobs, agent_rewards, mean_entropy, epoch, params)\n",
    "        replay.append((scores[agent_index], actor_losses, critic_losses, losses))\n",
    "\n",
    "    return scores, epsilon, reward_leadup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trjectory_loss(values, logprobs, rewards, mean_entropy, epoch, params):\n",
    "\n",
    "    reward_leadup = np.clip((params['end_reward_leadup'] - params['start_reward_leadup']) / (params['epochs'] - 0) * epoch + params['start_reward_leadup'], params['end_reward_leadup'], params['start_reward_leadup'])\n",
    "\n",
    "    [logprob0, logprob1, logprob2, logprob3] = logprobs\n",
    "\n",
    "    values = values.flip(dims=(0,))\n",
    "    rewards = torch.Tensor(rewards).flip(dims=(0,))\n",
    "    logprob0 = logprob0.flip(dims=(0,))\n",
    "    logprob1 = logprob1.flip(dims=(0,))\n",
    "    logprob2 = logprob2.flip(dims=(0,))\n",
    "    logprob3 = logprob3.flip(dims=(0,))\n",
    "\n",
    "    Returns = []\n",
    "    total_return = torch.Tensor([0])\n",
    "    leadup = 0\n",
    "\n",
    "    for reward_index in range(len(rewards)):\n",
    "        if rewards[reward_index].item() > 0:\n",
    "            leadup = reward_leadup\n",
    "        if leadup == 0:\n",
    "            total_return = torch.Tensor([0])\n",
    "        \n",
    "        total_return = rewards[reward_index] + total_return * params['gamma']\n",
    "        Returns.append(total_return)\n",
    "        leadup = leadup - 1 if leadup > 0 else 0\n",
    "\n",
    "    Returns = torch.stack(Returns).view(-1)\n",
    "    Returns = F.normalize(Returns, dim=0)\n",
    "\n",
    "    actor_loss0 = -logprob0 * (Returns - values.detach())\n",
    "    actor_loss1 = -logprob1 * (Returns - values.detach())\n",
    "    actor_loss2 = -logprob2 * (Returns - values.detach())\n",
    "    actor_loss3 = -logprob3 * (Returns - values.detach())\n",
    "\n",
    "    critic_loss = torch.pow(values - Returns, 2)\n",
    "\n",
    "    actor_loss0 = actor_loss0.sum()\n",
    "    actor_loss1 = actor_loss1.sum()\n",
    "    actor_loss2 = actor_loss2.sum()\n",
    "    actor_loss3 = actor_loss3.sum()\n",
    "\n",
    "    critic_loss = critic_loss.sum()\n",
    "\n",
    "    loss0 = actor_loss0 + params['clc']*critic_loss + 0.01 * mean_entropy\n",
    "    loss1 = actor_loss1 + params['clc']*critic_loss + 0.01 * mean_entropy\n",
    "    loss2 = actor_loss2 + params['clc']*critic_loss + 0.01 * mean_entropy\n",
    "    loss3 = actor_loss3 + params['clc']*critic_loss + 0.01 * mean_entropy\n",
    "\n",
    "    actor_losses = (actor_loss0, actor_loss1, actor_loss2, actor_loss3)\n",
    "    losses = (loss0, loss1, loss2, loss3)\n",
    "\n",
    "    return actor_losses, critic_loss, losses, reward_leadup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(replay, optimizer, params):\n",
    "    loss0 = torch.tensor(0.)\n",
    "    loss1 = torch.tensor(0.)\n",
    "    loss2 = torch.tensor(0.)\n",
    "    loss3 = torch.tensor(0.)\n",
    "    actor_loss0 = torch.tensor(0.)\n",
    "    actor_loss1 = torch.tensor(0.)\n",
    "    actor_loss2 = torch.tensor(0.)\n",
    "    actor_loss3 = torch.tensor(0.)\n",
    "    critic_loss = torch.tensor(0.)\n",
    "\n",
    "    for trajectory in replay:\n",
    "        rewards_sum, actor_losses, critic_loss, losses = trajectory\n",
    "        loss0 += losses[0]\n",
    "        loss1 += losses[1]\n",
    "        loss2 += losses[2]\n",
    "        loss3 += losses[3]\n",
    "        actor_loss0 += actor_losses[0]\n",
    "        actor_loss1 += actor_losses[1]\n",
    "        actor_loss2 += actor_losses[2]\n",
    "        actor_loss3 += actor_losses[3]\n",
    "        critic_loss += critic_loss\n",
    "    \n",
    "\n",
    "    loss0 = loss0 / len(replay)\n",
    "    loss1 = loss1 / len(replay)\n",
    "    loss2 = loss2 / len(replay)\n",
    "    loss3 = loss3 / len(replay)\n",
    "    actor_loss0 = actor_loss0 / len(replay)\n",
    "    actor_loss1 = actor_loss1 / len(replay)\n",
    "    actor_loss2 = actor_loss2 / len(replay)\n",
    "    actor_loss3 = actor_loss3 / len(replay)\n",
    "    critic_loss = critic_loss / len(replay)\n",
    "\n",
    "    loss_mean = (loss0 + loss1 + loss2 + loss3) / 4\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss_mean.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    actor_loss_sum = actor_loss0 + actor_loss1 + actor_loss2 + actor_loss3\n",
    "\n",
    "    return loss_mean, actor_loss_sum, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "worker(model, optimizer, params)\n",
    "save_model(model, optimizer, 'actor_critic.pt')\n",
    "end = perf_counter()\n",
    "print((end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(params['scores'], params['ave_scores'], filename='scores.png', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(params['losses'], 'loss.png', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(params['actor_losses'], filename='actor_loss.png', plotName=\"Actor Losses\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(params['critic_losses'], filename='critic_loss.png', plotName=\"Critic Losses\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
